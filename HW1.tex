\documentclass[a4paper,12pt]{article}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{fancyhdr}
\usepackage{geometry}
\geometry{margin=1in}
% Custom commands for vectors
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\begin{document}
\begin{center}
{\Large CS 534 ML Spring 2025 Homework 1}
\vspace{.8cm}
\begin{tabular}{rl}
\newline
Name: & [Felipe Ortega Cardozo] \\
Collaborators: & [None]
\end{tabular}
\end{center}
By turning in this assignment, I declare to adhere to the provisions of the Emory honor code and that all of this is my own work.
Disclaimer: AI was used to format the latex as the instructions require and for writing specific formulas and syntaxes that were not in the source file

/* THIS CODE IS MY OWN WORK, IT WAS WRITTEN WITHOUT
CONSULTING CODE WRITTEN BY OTHER STUDENTS
OR LARGE LANGUAGE MODELS LIKE CHATGPT.
Felipe Cardozo*/
I collaborated with the following classmates for this homework:
None
\section*{1. (Written) Ridge Regression (10 pts)}
Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix $\mathbf{X}$ with $k$ additional rows with the value $\sqrt{\lambda \mathbf{I}}$ and augment $\mathbf{y}$ with $k$ zeros. The idea is that by introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero.
\subsection*{Solution}
\textbf{Step 1: Define the Augmented Data.}
Let $\mathbf{X} \in \mathbb{R}^{N \times d}$ be the centered design matrix, $\mathbf{y} \in \mathbb{R}^{N \times 1}$ be the target vector, and $\lambda > 0$ be the regularization parameter. Define the augmented design matrix $\mathbf{X}' \in \mathbb{R}^{(N+d) \times d}$ and the augmented target vector $\mathbf{y}' \in \mathbb{R}^{(N+d) \times 1}$ as:
\[
\mathbf{X}' = \begin{bmatrix} \mathbf{X} \\ \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}, \qquad
\mathbf{y}' = \begin{bmatrix} \mathbf{y} \\ \mathbf{0}_d \end{bmatrix},
\]
where $\mathbf{I}_d$ is the $d \times d$ identity matrix and $\mathbf{0}_d$ is the $d \times 1$ zero vector.
\bigskip
\textbf{Step 2: State the OLS Estimator.}
The ordinary least squares (OLS) closed-form solution applied to the augmented data is:
\[
\hat{\beta}_{\text{OLS}} = \left(\mathbf{X}'^T \mathbf{X}'\right)^{-1} \mathbf{X}'^T \mathbf{y}'.
\]
\bigskip
\textbf{Step 3: Expand the Terms.}
\textit{Computing $\mathbf{X}'^T \mathbf{X}'$:}
\begin{align*}
\mathbf{X}'^T \mathbf{X}'
&= \begin{bmatrix} \mathbf{X}^T & \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}
   \begin{bmatrix} \mathbf{X} \\ \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix} \\
&= \mathbf{X}^T \mathbf{X} + \sqrt{\lambda}\, \mathbf{I}_d \cdot \sqrt{\lambda}\, \mathbf{I}_d \\
&= \mathbf{X}^T \mathbf{X} + \lambda\, \mathbf{I}_d.
\end{align*}
\textit{Computing $\mathbf{X}'^T \mathbf{y}'$:}
\begin{align*}
\mathbf{X}'^T \mathbf{y}'
&= \begin{bmatrix} \mathbf{X}^T & \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}
   \begin{bmatrix} \mathbf{y} \\ \mathbf{0}_d \end{bmatrix} \\
&= \mathbf{X}^T \mathbf{y} + \sqrt{\lambda}\, \mathbf{I}_d \cdot \mathbf{0}_d \\
&= \mathbf{X}^T \mathbf{y}.
\end{align*}
\bigskip
\textbf{Step 4: Conclusion.}
Substituting the expanded terms back into the OLS formula:
\begin{align*}
\hat{\beta}_{\text{OLS}}
&= \left(\mathbf{X}'^T \mathbf{X}'\right)^{-1} \mathbf{X}'^T \mathbf{y}' \\
&= \left(\mathbf{X}^T \mathbf{X} + \lambda\, \mathbf{I}_d\right)^{-1} \mathbf{X}^T \mathbf{y} \\
&= \hat{\beta}_{\text{Ridge}}.
\end{align*}
This shows that the OLS estimator on the augmented data $(\mathbf{X}', \mathbf{y}')$ is identical to the ridge regression estimator $\hat{\beta}_{\text{Ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I}_d)^{-1} \mathbf{X}^T \mathbf{y}$. Hence, ridge regression estimates can be obtained by ordinary least squares on the augmented data set. $\blacksquare$
\section*{2. Predicting Appliance Energy Usage using Linear Regression (40 pts)}
Consider the Appliances energy prediction dataset (\texttt{energydata.zip}), which contains measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station, and the recorded energy use of lighting fixtures to predict the energy consumption of appliances (\texttt{Appliances} attribute) in a low energy house. The data has been split into three subsets: training data from measurements up to 3/20/16 5:30, validation data from measurements between 3/20/16 5:30 and 5/7/16 4:30, and test data from measurements after 5/7/16 4:30. There are 26 attributes for each 10-minute interval, which is described in detail on the UCL ML repository, Appliances energy prediction dataset.
Your goal is to predict the \texttt{Appliances}. For this problem, you will use \texttt{scikit-learn} for linear regression, ridge regression, and lasso regression. All the specified functions should be in the file \texttt{'q2.py'}. The functions in \texttt{'q2.py'} will be tested against a different training, validation, and test set, so it should work for a variety of datasets and assume that the data has been appropriately pre-processed (i.e., do not do any standardization or scaling or anything to the data prior to training the model). Any additional work such as loading the file, plotting, and required analysis with the data (e.g., parts 2e, 2h, 2j, etc.) should be done in a separate file and submitted with the Code.
\subsection*{(a) (Written)}
How did you preprocess the data? Explain your reasoning for using this pre-processing.
\medskip


\noindent\textbf{Answer:} For preprocessing this dataset, I went with two straightforward steps that made sense based on what I saw in the data. First off, I dropped the 'date' column entirely, which are timestamps such as "1/11/16 17:00," or strings, and don't translate well into numerical features for regression models. Keeping it would have meant converting it to something usable, like extracting hour or day of week, but the problem didn't call for feature engineering, so it was safer to remove it to avoid complications.

Second, I applied z-score standardization to all the numerical features, meaning for each column, I subtracted the mean and divided by the standard deviation, but calculated the stats only from the training set. Then I used the same means and stds to transform the validation and test sets, so there's no leakage from future data into the training process.

The reason I chose standardization is pretty practical as ridge and lasso penalties work best when features are on similar scales, which we learned in class. In this dataset, temperatures range from about 15 to 30, while pressures are in the 730 to 775 ballpark, so without scaling, the model would unfairly shrink coefficients for larger-scale features more than others. By standardizing, everything's centered around zero with unit variance, so the regularization treats all features equally, which helps in getting a fairer and more stable model. I didn't go for min-max scaling because z-score handles outliers better in this context, and the features seemed normally distributed enough from a quick look.
\subsection*{(b) (Code)}
Write a Python function \texttt{preprocess\_data(trainx, valx, testx)} that does what you specified in 2a above. If you do any feature extraction, you should do it outside of this function. Your function should return the preprocessed \texttt{trainx}, \texttt{valx}, and \texttt{testx}.
\subsection*{(c) (Code)}
Write a Python function \texttt{eval\_linear1(trainx, trainy, valx, valy, testx, testy)} that takes in a training set, validation set, and test set, (in the form of numpy arrays), respectively, and trains a standard linear regression model only on the training data and reports the RMSE and $R^{2}$ on the training set, validation set, and test set. Your function must return a dictionary with 6 keys, \texttt{'train-rmse'}, \texttt{'train-r2'}, \texttt{'val-rmse'}, \texttt{'val-r2'}, \texttt{'test-rmse'}, \texttt{'test-r2'} and the associated values are the numeric values.
\subsection*{(d) (Code)}
Write a Python function \texttt{eval\_linear2(trainx, trainy, valx, valy, testx, testy)} that takes in a training set, validation set, and test set, respectively, and trains a standard linear regression model using the training and validation data together and reports the RMSE and $R^{2}$ on the training set, validation set, and test set. Your function should follow the same output format specified above.
\subsection*{(e) (Written)}
Report (using a table) the RMSE and $R^{2}$ between 2c and 2d on the \texttt{energydata}. How do the performances compare and what do the numbers suggest?
\medskip

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
LR1 (train only) & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
LR2 (train + val) & 99.38 & 0.1676 & 88.71 & 0.1751 & 86.41 & 0.0959 \\
\hline
\end{tabular}%
}
\end{center}


When I ran these, LR1, which was trained just on the training data, did slightly better on its own set with a lower RMSE of 98.23 compared to LR2's 99.38. But on validation and test, it's a different story because validation R² is basically zero at 0.0027, and test R² is negative at -0.21, meaning it's worse than just guessing the average value every time.

However, LR2, by combining train and val for training, sacrifices a little on train fit but gains a lot elsewhere. Test RMSE improves from 100.03 to 86.41, and R² flips to positive 0.096, showing how much more data helps, and how the validation set adds diversity that makes the model generalize better. The poor test performance in LR1 suggests the training data alone led to some overfitting to specific patterns that don't hold up later. Adding val data smooths that out, reducing variance and giving a more reliable model overall. In other words, more data often beats tweaking the model when the dataset isn't huge.
\subsection*{(f) (Code)}
Write a Python function \texttt{eval\_ridge1(trainx, trainy, valx, valy, testx, testy, alpha)} that takes the regularization parameter, \texttt{alpha}, and trains a ridge regression model only on the training data. Your function should follow the same output format specified in (a) and (b).
\subsection*{(g) (Code)}
Write a Python function \texttt{eval\_lasso1(trainx, trainy, valx, valy, testx, testy, alpha)} that takes the regularization parameter, \texttt{alpha}, and trains a lasso regression model only on the training data. Your function should follow the same output format specified in (a), (b), and (d).
\subsection*{(h) (Written)}
Report (using a table) the RMSE and $R^{2}$ for training, validation, and test for all the different ($\lambda$) values you tried. What would be the optimal parameter you would select based on the validation data performance?
\medskip
\noindent\textbf{Answer:}
\noindent\textbf{Ridge Regression} ($\lambda$ values from $10^{-4}$ to $10^{2}$, trained on training set only):
\begin{center}
\small
\begin{tabular}{r r r r r r r}
\hline
$\lambda$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.0001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.01 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.1 & 98.23 & 0.1868 & 97.55 & 0.0025 & 100.05 & $-0.2121$ \\
1.0 & 98.23 & 0.1867 & 97.61 & 0.0013 & 100.19 & $-0.2156$ \\
10.0 & 98.24 & 0.1865 & 98.03 & $-0.0075$ & 101.19 & $-0.2398$ \\
31.62 & 98.27 & 0.1861 & 98.27 & $-0.0124$ & 101.70 & $-0.2526$ \\
100.0 & 98.35 & 0.1848 & 97.89 & $-0.0044$ & 100.57 & $-0.2247$ \\
\hline
\end{tabular}
\end{center}
\noindent\textbf{Lasso Regression} ($\lambda$ values from $10^{-4}$ to $10^{2}$, trained on training set only):
\begin{center}
\small
\begin{tabular}{r r r r r r r}
\hline
$\lambda$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.0001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.001 & 98.23 & 0.1868 & 97.55 & 0.0024 & 100.06 & $-0.2123$ \\
0.01 & 98.23 & 0.1867 & 97.68 & $-0.0001$ & 100.34 & $-0.2192$ \\
0.1 & 98.29 & 0.1858 & 98.51 & $-0.0172$ & 101.99 & $-0.2597$ \\
0.5623 & 98.54 & 0.1817 & 97.95 & $-0.0058$ & 100.15 & $-0.2146$ \\
1.0 & 98.84 & 0.1766 & 96.53 & 0.0232 & 96.22 & $-0.1212$ \\
\textbf{3.1623} & \textbf{100.87} & \textbf{0.1424} & \textbf{93.81} & \textbf{0.0776} & \textbf{88.70} & \textbf{0.0472} \\
5.6234 & 103.45 & 0.0981 & 95.27 & 0.0485 & 90.05 & 0.0181 \\
10.0 & 104.84 & 0.0736 & 96.24 & 0.0290 & 89.47 & 0.0306 \\
31.62 & 108.93 & 0.0000 & 97.68 & $-0.0003$ & 90.89 & $-0.0003$ \\
100.0 & 108.93 & 0.0000 & 97.68 & $-0.0003$ & 90.89 & $-0.0003$ \\
\hline
\end{tabular}
\end{center}
\noindent\textbf{Optimal parameters based on validation RMSE:}
\begin{itemize}
\item \textbf{Ridge:} I picked $\lambda_{\text{Ridge}}^{*} = 0.0001$ because it gave the lowest val RMSE of 97.54. Basically, for ridge, adding more regularization didn't help; the smallest lambda, which is almost like no regularization, worked best here on the training data alone. It tells me that ridge isn't adding much value beyond plain OLS in this setup.
\item \textbf{Lasso:} For lasso, $\lambda_{\text{Lasso}}^{*} = 3.1623$ stood out with val RMSE 93.81, way better than OLS or ridge because lasso zeros out noisy features, 16 of them in this case, focusing on the important ones and boosting generalization. It shows there are redundant or irrelevant variables in the mix, and lasso's sparsity helps clean that up.
\end{itemize}
\subsection*{(i) (Code)}
Similar to part 2d, write the Python functions, \texttt{eval\_ridge2(trainx, trainy, valx, valy, testx, testy, alpha)} and \texttt{eval\_lasso2(trainx, trainy, valx, valy, testx, testy, alpha)} that train ridge and lasso using the training and validation set.

\subsection*{(j) (Written)}
Use the optimal regularization parameter from 2h and report the RMSE and $R^{2}$ on the training set, validation set, and test set for the functions you wrote on 2i? How does this compare to the results from 2h? What do the numbers suggest?
\medskip

\noindent\textbf{Answer:}
Using the optimal $\lambda$ from 2h ($\lambda_{\text{Ridge}}^{*} = 0.0001$, $\lambda_{\text{Lasso}}^{*} = 3.1623$), the models are now retrained on the combined training + validation set:


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
Ridge1 ($\lambda=0.0001$)    & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
Ridge2 ($\lambda=0.0001$)    & 99.38 & 0.1676 & 88.71 & 0.1751 &  86.41 &    0.0959 \\
\hline
Lasso1 ($\lambda=3.1623$)    & 100.87 & 0.1424 & 93.81 & 0.0776 & 88.70 & 0.0472 \\
Lasso2 ($\lambda=3.1623$)    & 101.67 & 0.1289 & 91.82 & 0.1163 & 86.64 & 0.0909 \\
\hline
\end{tabular}%
}
\end{center}


Retraining on the bigger set (train + val) really boosted things for both. For ridge, test RMSE dropped from 100.03 to 86.41, and R² went from negative to 0.10 clearly, more data helped it capture broader patterns without overfitting as much.

Lasso saw a similar lift, test RMSE from 88.70 to 86.64 and R² up to 0.091, but the gain was smaller since it was already doing okay with feature selection. Train RMSE went up a tad in both cases, which makes sense because the model's not tailoring itself as tightly to the original train data.

Overall, this screams bias-variance tradeoff: you're accepting a slight worse fit on train for much better performance elsewhere. Interesting how ridge and lasso end up neck-and-neck on test with more data; it suggests that once you have enough samples, the type of regularization matters less than just having the volume to learn robustly.


\subsection*{(k) (Written)}
Generate the coefficient path plots (regularization value vs. coefficient value) for both ridge and lasso. Also, note (line or point or star) where the optimal regularization parameters from 2h are on their respective plots. Make sure that your plots encompass all the expected behavior (coefficients should shrink towards 0).

\medskip
\noindent\textbf{Answer:}
The coefficient path plots were generated for $\lambda \in [10^{-4}, 10^{2}]$ using 50 log-spaced values. The optimal $\lambda$ from 2h is marked on each plot with a star. 

\textbf{Ridge coefficient path:} As $\lambda$ increases from $10^{-4}$ to $10^{2}$, all 25 coefficients shrink continuously towards zero, but none are ever exactly zero. At low $\lambda$, the dominant features are \texttt{T\_out} ($-67.3$), \texttt{Tdewpoint} ($+63.3$), \texttt{RH\_2} ($-60.3$), \texttt{RH\_3} ($+53.7$), and \texttt{RH\_1} ($+43.4$). These remain the primary coefficients across the path, though their magnitudes diminish, and by $\lambda = 100$, the largest remaining coefficients are \texttt{RH\_2} ($-47.8$), \texttt{RH\_3} ($+44.3$), with \texttt{RH\_1} ($+36.7$), while \texttt{T\_out} has shrunk to $-8.1$ and \texttt{Tdewpoint} to $+6.6$.

Thus, The optimal $\lambda^*_{\text{Ridge}} = 0.0001$ lies at the far left, where regularization has negligible impact.

\textbf{Lasso coefficient path:} Lasso exhibits sparsity by driving coefficients to exactly zero at distinct thresholds. At $\lambda = 10^{-4}$, all 25 features are active. By $\lambda \approx 0.087$, two features are eliminated. At the optimal $\lambda^*_{\text{Lasso}} = 3.1623$, 16 out of 25 features are zeroed out. The 9 surviving features are: \texttt{lights}, \texttt{RH\_1}, \texttt{RH\_2}, \texttt{T3}, \texttt{RH\_3}, \texttt{T5}, \texttt{T6}, \texttt{RH\_8}, and \texttt{RH\_out}. For $\lambda \geq 31.62$, all coefficients are zeroed, resulting in a model that predicts only the target mean.


\subsection*{(l) (Written)}
What are 3 observations you can draw from looking at the coefficient path plots, and the metrics? This should be different from your observations from 2e, 2h, and 2j.

\medskip
\noindent\textbf{Answer:}
\begin{enumerate}
    \item \textbf{Implicit Feature Selection:} Lasso performs automated feature selection by zeroing out coefficients, whereas Ridge only performs shrinkage. At the optimal Lasso $\lambda$, only 9 features remain, yet it outperforms the dense Ridge model on validation metrics,  showing that a sparse model can effectively reduce noise and improve generalization when the dataset contains irrelevant features.
    
    \item \textbf{Handling of Correlated Features:} Features such as \texttt{T\_out} and \texttt{Tdewpoint} are physically correlated. In the Ridge path, both retain moderate coefficients, sharing the predictive load. In contrast, Lasso zeros both out by $\lambda = 3.1623$ in favor of humidity features (\texttt{RH\_1}, \texttt{RH\_2}, \texttt{RH\_3}), showing Lasso's tendency to select a representative feature from a correlated group, whereas Ridge distributes weights among them.
    
    \item \textbf{Feature Importance Ranking:} The order in which Lasso eliminates features provides an interpretable importance ranking. Survivors like \texttt{lights} and \texttt{RH\_1} are individually strong predictors. Early dropouts like \texttt{Windspeed} and \texttt{Visibility} have weak predictive power. This ranking is a unique diagnostic tool for interpretation that is not available in standard Ridge or OLS regression.
\end{enumerate}


\section*{3. Predicting Appliance Energy Usage using SGD (50 pts)}
Consider the Appliances energy prediction Data set from the previous problem. A template file, \texttt{elastic.py}, defines a class \texttt{ElasticNet} that takes in the regularization parameters, \texttt{el} ($\lambda$), \texttt{alpha} ($\alpha$), \texttt{eta} ($\eta$) or the learning rate, the \texttt{batch} size (batch $\in [1, N]$), and \texttt{epoch} or the maximum number of epochs as parameters when creating the object (i.e., \texttt{elastic = new ElasticNet(el, alpha, eta, batch, epoch)}). You will implement \texttt{ElasticNet} using stochastic gradient descent to train your model. The functions in \texttt{'elastic.py'} will be tested against a different training, validation, and test set, so it should work for a variety of datasets and assume that the data has been appropriately pre-processed. For this problem, you \textbf{ARE NOT} allowed to use any existing toolbox/implementation (e.g., \texttt{scikit-learn}). Similar to problem 2, any additional work outside of (Code) should be done in a separate file and submitted with the Code for full credit.
\subsection*{(a) (Code)}
Implement the loss objective helper function in \texttt{elastic.py}. As a reminder, the optimization problem is:
$$
\min f_{o}(x) = \frac{1}{2} ||\mathbf{y} - \mathbf{X}\beta||_{2}^{2} + \lambda(\alpha||\beta||_{2}^{2} + (1-\alpha)||\beta||_{1}), \quad 0 < \alpha < 1 \quad (1)
$$
In other words, given the coefficients, data and the regularization parameters, your function will calculate the loss $f_{o}(x)$ as shown in Eq (1).
\subsection*{(b) (Code)}
Implement the gradient helper function in \texttt{elastic.py}. You may find it helpful to derive the update for a single training sample and to consider proximal gradient descent for the $||\beta||_{1}$ portion of the objective function. As a reminder, given step size $\eta$ and regularization parameter $\lambda$ such that $f(x) = g(x) + \lambda||x||_{1}$, the proximal update is:
$$
\text{prox}(x_i) =
\begin{cases}
x_i - \lambda\eta & \text{if } x_i > \lambda\eta \\
0 & \text{if } |x_i| \le \lambda\eta \\
x_i + \lambda\eta & \text{if } x_i < -\lambda\eta
\end{cases}
$$
\subsection*{(c) (Code)}
Implement the Python function \texttt{train(self, x, y)} for your class that trains an elastic net regression model using stochastic gradient descent. Your function should return a dictionary where the key denotes the epoch number and the value of the loss associated with that epoch. (Note: lambda is not used since it is a Python function and can cause confusion.)
\subsection*{(d) (Code)}
Implement the Python function \texttt{coef(self)} for your class that returns the learned coefficients as a numpy array.
\subsection*{(e) (Code)}
Implement the Python function \texttt{predict(self, x)} that predicts the label for each training sample in x. If x is a numpy $m \times d$ array, then y is a numpy 1-d array of size $m \times 1$.
\subsection*{(f) (Written)}
For the optimal regularization parameters from ridge $(\lambda_{ridge})$ and lasso $(\lambda_{lasso})$ from 2h, and $\alpha = \frac{1}{2}$, what are good learning rates for the dataset? Justify the selection by trying various learning rates and illustrating the objective value $(f_{o}(x))$ on a graph for a range of epochs (one epoch = one pass through the training data). For the chosen learning rate you identified, what are the RMSE and $R^{2}$ for the elastic net model trained on the entire training set on the training, validation, and test sets?
\medskip
\noindent\textbf{Answer:}
I used lambda from lasso optimal, 3.1623, with alpha=0.5, batch=32, and ran 500 epochs. Tested learning rates from 10^{-3} down to 10^{-7}. Here's the table from my runs:
\begin{center}
\small
\begin{tabular}{r r r r r r r r}
\hline
$\eta$ & Loss (ep 0) & Loss (ep 499) & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE \\
\hline
$10^{-3}$ & $9.92 \times 10^7$ & $9.75 \times 10^7$ & 140.59 & $-0.67$ & 126.86 & $-0.69$ & 101.48 \\
$10^{-4}$ & $9.99 \times 10^7$ & $9.67 \times 10^7$ & 140.01 & $-0.65$ & 130.24 & $-0.78$ & 113.62 \\
$10^{-5}$ & $1.05 \times 10^8$ & $9.67 \times 10^7$ & 139.99 & $-0.65$ & 131.17 & $-0.80$ & 116.99 \\
$10^{-6}$ & $1.06 \times 10^8$ & $9.74 \times 10^7$ & 140.53 & $-0.66$ & 133.38 & $-0.86$ & 123.32 \\
$\mathbf{10^{-7}}$ & $1.06 \times 10^8$ & $1.01 \times 10^8$ & 143.41 & $-0.73$ & \textbf{125.40} & $-0.65$ & 109.04 \\
\hline
\end{tabular}
\end{center}
\noindent\textbf{Chosen learning rate: $\eta = 10^{-7}$.}
I went with 10^{-7} because it gave the best val RMSE at 125.40. When I plotted the loss over epochs, it decreased steadily without jumping around:
\begin{center}
\small
\begin{tabular}{r r}
\hline
Epoch & Objective $f_o$ \\
\hline
0 & $1.064 \times 10^8$ \\
49 & $1.054 \times 10^8$ \\
99 & $1.047 \times 10^8$ \\
199 & $1.035 \times 10^8$ \\
499 & $1.013 \times 10^8$ \\
\hline
\end{tabular}
\end{center}
Bigger etas like 10^{-3} dropped loss faster but the updates were noisy from mini-batches, leading to wobbly paths and worse final metrics. Since our loss is sum-based (not averaged), gradients are huge, and the Lipschitz constant is about 88,779, so theoretical max eta is around 2.3e-5 for stability in full GD. With SGD, noise makes it trickier, and smaller eta gave smoother progress. For the final run on full train with this eta:
\begin{center}
\begin{tabular}{l r r}
\hline
\textbf{Set} & \textbf{RMSE} & $\mathbf{R^2}$ \\
\hline
Train & 143.41 & $-0.7334$ \\
Validation & 125.40 & $-0.6484$ \\
Test & 109.04 & $-0.4399$ \\
\hline
\end{tabular}
\end{center}

Those negative R² show it's not converged yet after 500 epochs, the condition number 8,282 means GD needs tons of steps, but Sklearn's exact solvers work instantly, so this highlights SGD's slowness here.

\subsection*{(g) (Written)}
Using the learning rate from the previous part, train elastic net (using only training data) for different values of $\alpha$ (it should encompass the entire range and include $\alpha = 0, 1$). Report the RMSE and $R^{2}$ for the models on training, validation, and test set.
\medskip
\noindent\textbf{Answer:}
With eta=10^{-7}, lambda=3.1623, batch=32, 500 epochs, I trained across alphas from 0 to 1:
\begin{center}
\begin{tabular}{r r r r r r r}
\hline
$\alpha$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.00 (pure $L_1$) & 143.37 & $-0.7323$ & 125.33 & $-0.6466$ & 108.90 & $-0.4361$ \\
0.10 & 143.38 & $-0.7325$ & 125.35 & $-0.6470$ & 108.93 & $-0.4369$ \\
0.25 & 143.39 & $-0.7329$ & 125.37 & $-0.6475$ & 108.97 & $-0.4380$ \\
0.50 & 143.41 & $-0.7334$ & 125.40 & $-0.6484$ & 109.04 & $-0.4399$ \\
0.75 & 143.43 & $-0.7339$ & 125.43 & $-0.6493$ & 109.12 & $-0.4418$ \\
0.90 & 143.44 & $-0.7342$ & 125.45 & $-0.6498$ & 109.16 & $-0.4429$ \\
1.00 (pure $L_2$) & 143.45 & $-0.7344$ & 125.47 & $-0.6501$ & 109.19 & $-0.4437$ \\
\hline
\end{tabular}
\end{center}


Results are super similar across alphas. Test RMSE barely moves from 108.90 at alpha=0 to 109.19 at 1, and Alpha=0 edges out with best val and test, but it's low.

This flatness is because the model hasn't converged far, and betas are small (~15 norm) vs sklearn's 45-146. When betas are tiny, L1 and L2 penalties don't differ much since penalties scale with beta size, then all negative R² confirm more epochs needed for SGD to catch up.
\subsection*{(h) (Written)}
Based on the results from (c) and 2(a) and 2(c), what conclusions can you draw in terms of RMSE and $R^{2}$? Which model is the best? Also, discuss the differences between the SGD-variants of Ridge and LASSO and the standard implementations (Problem 2).
\medskip
\noindent\textbf{Answer:}
\begin{center}
\adjustbox{max width=\textwidth,center}
{
\footnotesize
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
sklearn Ridge ($\lambda=0.0001$) & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
sklearn Lasso ($\lambda=3.1623$) & 100.87 & 0.1424 & 93.81 & 0.0776 & 88.70 & 0.0472 \\
SGD $\alpha=1$ (Ridge-like) & 143.45 & $-0.7344$ & 125.47 & $-0.6501$ & 109.19 & $-0.4437$ \\
SGD $\alpha=0$ (Lasso-like) & 143.37 & $-0.7323$ & 125.33 & $-0.6466$ & 108.90 & $-0.4361$ \\
SGD $\alpha=0.5$ (Elastic Net) & 143.41 & $-0.7334$ & 125.40 & $-0.6484$ & 109.04 & $-0.4399$ \\
\hline
\end{tabular}
}
\end{center}
\noindent\textbf{Best model:} Scikit-learn’s Lasso model at λ = 3.1623 delivers the best performance, achieving the lowest test RMSE of 88.70 and a positive R² of 0.047, along with strong validation results. Overall, it clearly outperforms SGD across all evaluated metrics.

\noindent\textbf{Key differences between SGD and sklearn:}
\begin{enumerate}
\item Loss setup: My SGD uses sum of squares for the data term, so with N=9868, it's huge (~10^8), making lambda tiny in effect. Sklearn averages by 1/(2N), so same alpha hits harder relative to data loss (~10^4).
\item Optimizer: Sklearn ridge is exact with matrix inverse, one shot. Lasso uses coordinate descent, quick exact solves per feature. My SGD is noisy from batches, proximal steps, fixed eta limits precision.
\item Conditioning: X^TX condition ~8282, SGD needs O(kappa log eps) steps, slow; coord descent handles sparsity better, less affected.
\item Where it lands: SGD betas ~15 norm, tiny vs sklearn's 44-146. Underfit, near-zero predictions, bad negative R².
\end{enumerate}
Bottom line, for this smallish data (N<10k, 25 feats), exact methods like sklearn are faster and spot-on. SGD shines for massive data where you can't load everything, but here it's overkill and underperforms without tweaks like scheduling eta or more epochs.
\subsection*{(i) (Written)}
What are the final coefficients that yield the best elastic net model on the test data? Compare these with the final coefficients for the best-performing model on the validation dataset. Are there noticeable differences? If so, discuss the differences with respect to the impact on the performance.
\medskip
\noindent\textbf{Answer:}
The best on test and val both ended up being alpha=0 (pure L1), and since same params and seed, identical model and coeffs:
\begin{center}
\small
\begin{tabular}{l r | l r}
\hline
\textbf{Feature} & \textbf{Coefficient} & \textbf{Feature} & \textbf{Coefficient} \\
\hline
lights & 10.4483 & T6 & 2.2838 \\
T1 & 0.6884 & RH\_6 & $-0.7428$ \\
RH\_1 & 3.2186 & T7 & $-0.3003$ \\
T2 & 4.1325 & RH\_7 & $-2.3318$ \\
RH\_2 & $-2.9227$ & T8 & 0.6047 \\
T3 & 3.5179 & RH\_8 & $-4.4550$ \\
RH\_3 & 2.1658 & T9 & $-1.2969$ \\
T4 & 1.1150 & RH\_9 & $-3.0644$ \\
RH\_4 & 0.0234 & T\_out & 1.7445 \\
T5 & $-1.1690$ & Press\_mm\_hg & $-0.7695$ \\
RH\_5 & $-0.7227$ & RH\_out & $-3.6775$ \\
Visibility & 0.0047 & Windspeed & 0.9500 \\
Tdewpoint & $-0.3360$ & & \\
\hline
\end{tabular}
\end{center}
Norm ~15.05, all 25 non-zero.

\noindent\textbf{Analysis:}
No difference between test-best and val-best since they use the same model. However, stacking against sklearn exposes the shortcomings:
\begin{itemize}
\item Size: SGD coeffs max ~10.4, tiny vs sklearn lasso ~24.1 or OLS ~63.3. Not enough epochs to build up.
\item Sparsity: Even at alpha=0, no zeros in SGD; sklearn zeros 16 as small coefficients mean proximal doesn't threshold much yet.
\item Rankings: Similar top features, lights are a strong positive and RH is key, but SGD still assigns weight to sklearn-zeroed variables (e.g., T_out, Tdewpoint, Press_mm_hg), introducing noise.
\item Performance hit: This half-baked state yields a poor fit (small coefficients) and no sparsity benefit, with worse RMSE (108.9 vs 88.7). More epochs or a decaying learning rate (η) could close the gap to the sklearn baseline.
\end{itemize}
\end{document}


