\documentclass[a4paper,12pt]{article}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{fancyhdr}
\usepackage{geometry}
\geometry{margin=1in}
% Custom commands for vectors
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\begin{document}
\begin{center}
{\Large CS 534 ML Spring 2025 Homework 1}
\vspace{.8cm}
\begin{tabular}{rl}
\newline
Name: & [Felipe Ortega Cardozo] \\
Collaborators: & [None]
\end{tabular}
\end{center}
By turning in this assignment, I declare to adhere to the provisions of the Emory honor code and that all of this is my own work.
Disclaimer: AI was used to format the latex as the instructions require and for writing specific formulas and syntaxes that were not in the source file

/* THIS CODE IS MY OWN WORK, IT WAS WRITTEN WITHOUT
CONSULTING CODE WRITTEN BY OTHER STUDENTS
OR LARGE LANGUAGE MODELS LIKE CHATGPT.
Felipe Cardozo*/
I collaborated with the following classmates for this homework:
None
\section*{1. (Written) Ridge Regression (10 pts)}
Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix $\mathbf{X}$ with $k$ additional rows with the value $\sqrt{\lambda \mathbf{I}}$ and augment $\mathbf{y}$ with $k$ zeros. The idea is that by introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero.
\subsection*{Solution}
\textbf{Step 1: Define the Augmented Data.}
Let $\mathbf{X} \in \mathbb{R}^{N \times d}$ be the centered design matrix, $\mathbf{y} \in \mathbb{R}^{N \times 1}$ be the target vector, and $\lambda > 0$ be the regularization parameter. Define the augmented design matrix $\mathbf{X}' \in \mathbb{R}^{(N+d) \times d}$ and the augmented target vector $\mathbf{y}' \in \mathbb{R}^{(N+d) \times 1}$ as:
\[
\mathbf{X}' = \begin{bmatrix} \mathbf{X} \\ \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}, \qquad
\mathbf{y}' = \begin{bmatrix} \mathbf{y} \\ \mathbf{0}_d \end{bmatrix},
\]
where $\mathbf{I}_d$ is the $d \times d$ identity matrix and $\mathbf{0}_d$ is the $d \times 1$ zero vector.
\bigskip
\textbf{Step 2: State the OLS Estimator.}
The ordinary least squares (OLS) closed-form solution applied to the augmented data is:
\[
\hat{\beta}_{\text{OLS}} = \left(\mathbf{X}'^T \mathbf{X}'\right)^{-1} \mathbf{X}'^T \mathbf{y}'.
\]
\bigskip
\textbf{Step 3: Expand the Terms.}
\textit{Computing $\mathbf{X}'^T \mathbf{X}'$:}
\begin{align*}
\mathbf{X}'^T \mathbf{X}'
&= \begin{bmatrix} \mathbf{X}^T & \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}
   \begin{bmatrix} \mathbf{X} \\ \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix} \\
&= \mathbf{X}^T \mathbf{X} + \sqrt{\lambda}\, \mathbf{I}_d \cdot \sqrt{\lambda}\, \mathbf{I}_d \\
&= \mathbf{X}^T \mathbf{X} + \lambda\, \mathbf{I}_d.
\end{align*}
\textit{Computing $\mathbf{X}'^T \mathbf{y}'$:}
\begin{align*}
\mathbf{X}'^T \mathbf{y}'
&= \begin{bmatrix} \mathbf{X}^T & \sqrt{\lambda}\, \mathbf{I}_d \end{bmatrix}
   \begin{bmatrix} \mathbf{y} \\ \mathbf{0}_d \end{bmatrix} \\
&= \mathbf{X}^T \mathbf{y} + \sqrt{\lambda}\, \mathbf{I}_d \cdot \mathbf{0}_d \\
&= \mathbf{X}^T \mathbf{y}.
\end{align*}
\bigskip
\textbf{Step 4: Conclusion.}
Substituting the expanded terms back into the OLS formula:
\begin{align*}
\hat{\beta}_{\text{OLS}}
&= \left(\mathbf{X}'^T \mathbf{X}'\right)^{-1} \mathbf{X}'^T \mathbf{y}' \\
&= \left(\mathbf{X}^T \mathbf{X} + \lambda\, \mathbf{I}_d\right)^{-1} \mathbf{X}^T \mathbf{y} \\
&= \hat{\beta}_{\text{Ridge}}.
\end{align*}
This shows that the OLS estimator on the augmented data $(\mathbf{X}', \mathbf{y}')$ is identical to the ridge regression estimator $\hat{\beta}_{\text{Ridge}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I}_d)^{-1} \mathbf{X}^T \mathbf{y}$. Hence, ridge regression estimates can be obtained by ordinary least squares on the augmented data set. $\blacksquare$
\section*{2. Predicting Appliance Energy Usage using Linear Regression (40 pts)}
Consider the Appliances energy prediction dataset (\texttt{energydata.zip}), which contains measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station, and the recorded energy use of lighting fixtures to predict the energy consumption of appliances (\texttt{Appliances} attribute) in a low energy house. The data has been split into three subsets: training data from measurements up to 3/20/16 5:30, validation data from measurements between 3/20/16 5:30 and 5/7/16 4:30, and test data from measurements after 5/7/16 4:30. There are 26 attributes for each 10-minute interval, which is described in detail on the UCL ML repository, Appliances energy prediction dataset.
Your goal is to predict the \texttt{Appliances}. For this problem, you will use \texttt{scikit-learn} for linear regression, ridge regression, and lasso regression. All the specified functions should be in the file \texttt{'q2.py'}. The functions in \texttt{'q2.py'} will be tested against a different training, validation, and test set, so it should work for a variety of datasets and assume that the data has been appropriately pre-processed (i.e., do not do any standardization or scaling or anything to the data prior to training the model). Any additional work such as loading the file, plotting, and required analysis with the data (e.g., parts 2e, 2h, 2j, etc.) should be done in a separate file and submitted with the Code.
\subsection*{(a) (Written)}
How did you preprocess the data? Explain your reasoning for using this pre-processing.
\medskip


\noindent\textbf{Answer:} For preprocessing this dataset, I went with two straightforward steps that made sense based on what I saw in the data. First off, I dropped the 'date' column entirely, which are timestamps such as "1/11/16 17:00," or strings, and don't translate well into numerical features for regression models. Keeping it would have meant converting it to something usable, like extracting hour or day of week, but the problem didn't call for feature engineering, so it was safer to remove it to avoid complications.

Second, I applied z-score standardization to all the numerical features, meaning for each column, I subtracted the mean and divided by the standard deviation, but calculated the stats only from the training set. Then I used the same means and stds to transform the validation and test sets, so there's no leakage from future data into the training process.

The reason I chose standardization is pretty practical as ridge and lasso penalties work best when features are on similar scales, which we learned in class. In this dataset, temperatures range from about 15 to 30, while pressures are in the 730 to 775 ballpark, so without scaling, the model would unfairly shrink coefficients for larger-scale features more than others. By standardizing, everything's centered around zero with unit variance, so the regularization treats all features equally, which helps in getting a fairer and more stable model. I didn't go for min-max scaling because z-score handles outliers better in this context, and the features seemed normally distributed enough from a quick look.
\subsection*{(b) (Code)}
Write a Python function \texttt{preprocess\_data(trainx, valx, testx)} that does what you specified in 2a above. If you do any feature extraction, you should do it outside of this function. Your function should return the preprocessed \texttt{trainx}, \texttt{valx}, and \texttt{testx}.
\subsection*{(c) (Code)}
Write a Python function \texttt{eval\_linear1(trainx, trainy, valx, valy, testx, testy)} that takes in a training set, validation set, and test set, (in the form of numpy arrays), respectively, and trains a standard linear regression model only on the training data and reports the RMSE and $R^{2}$ on the training set, validation set, and test set. Your function must return a dictionary with 6 keys, \texttt{'train-rmse'}, \texttt{'train-r2'}, \texttt{'val-rmse'}, \texttt{'val-r2'}, \texttt{'test-rmse'}, \texttt{'test-r2'} and the associated values are the numeric values.
\subsection*{(d) (Code)}
Write a Python function \texttt{eval\_linear2(trainx, trainy, valx, valy, testx, testy)} that takes in a training set, validation set, and test set, respectively, and trains a standard linear regression model using the training and validation data together and reports the RMSE and $R^{2}$ on the training set, validation set, and test set. Your function should follow the same output format specified above.
\subsection*{(e) (Written)}
Report (using a table) the RMSE and $R^{2}$ between 2c and 2d on the \texttt{energydata}. How do the performances compare and what do the numbers suggest?
\medskip

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
LR1 (train only) & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
LR2 (train + val) & 99.38 & 0.1676 & 88.71 & 0.1751 & 86.41 & 0.0959 \\
\hline
\end{tabular}%
}
\end{center}


When I ran these, LR1, which was trained just on the training data, did slightly better on its own set with a lower RMSE of 98.23 compared to LR2's 99.38. But on validation and test, it's a different story because validation R² is basically zero at 0.0027, and test R² is negative at -0.21, meaning it's worse than just guessing the average value every time.

However, LR2, by combining train and val for training, sacrifices a little on train fit but gains a lot elsewhere. Test RMSE improves from 100.03 to 86.41, and R² flips to positive 0.096, showing how much more data helps, and how the validation set adds diversity that makes the model generalize better. The poor test performance in LR1 suggests the training data alone led to some overfitting to specific patterns that don't hold up later. Adding val data smooths that out, reducing variance and giving a more reliable model overall. In other words, more data often beats tweaking the model when the dataset isn't huge.
\subsection*{(f) (Code)}
Write a Python function \texttt{eval\_ridge1(trainx, trainy, valx, valy, testx, testy, alpha)} that takes the regularization parameter, \texttt{alpha}, and trains a ridge regression model only on the training data. Your function should follow the same output format specified in (a) and (b).
\subsection*{(g) (Code)}
Write a Python function \texttt{eval\_lasso1(trainx, trainy, valx, valy, testx, testy, alpha)} that takes the regularization parameter, \texttt{alpha}, and trains a lasso regression model only on the training data. Your function should follow the same output format specified in (a), (b), and (d).
\subsection*{(h) (Written)}
Report (using a table) the RMSE and $R^{2}$ for training, validation, and test for all the different ($\lambda$) values you tried. What would be the optimal parameter you would select based on the validation data performance?
\medskip
\noindent\textbf{Answer:}
\noindent\textbf{Ridge Regression} ($\lambda$ values from $10^{-4}$ to $10^{2}$, trained on training set only):
\begin{center}
\small
\begin{tabular}{r r r r r r r}
\hline
$\lambda$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.0001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.01 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.1 & 98.23 & 0.1868 & 97.55 & 0.0025 & 100.05 & $-0.2121$ \\
1.0 & 98.23 & 0.1867 & 97.61 & 0.0013 & 100.19 & $-0.2156$ \\
10.0 & 98.24 & 0.1865 & 98.03 & $-0.0075$ & 101.19 & $-0.2398$ \\
31.62 & 98.27 & 0.1861 & 98.27 & $-0.0124$ & 101.70 & $-0.2526$ \\
100.0 & 98.35 & 0.1848 & 97.89 & $-0.0044$ & 100.57 & $-0.2247$ \\
\hline
\end{tabular}
\end{center}
\noindent\textbf{Lasso Regression} ($\lambda$ values from $10^{-4}$ to $10^{2}$, trained on training set only):
\begin{center}
\small
\begin{tabular}{r r r r r r r}
\hline
$\lambda$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.0001 & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
0.001 & 98.23 & 0.1868 & 97.55 & 0.0024 & 100.06 & $-0.2123$ \\
0.01 & 98.23 & 0.1867 & 97.68 & $-0.0001$ & 100.34 & $-0.2192$ \\
0.1 & 98.29 & 0.1858 & 98.51 & $-0.0172$ & 101.99 & $-0.2597$ \\
0.5623 & 98.54 & 0.1817 & 97.95 & $-0.0058$ & 100.15 & $-0.2146$ \\
1.0 & 98.84 & 0.1766 & 96.53 & 0.0232 & 96.22 & $-0.1212$ \\
\textbf{3.1623} & \textbf{100.87} & \textbf{0.1424} & \textbf{93.81} & \textbf{0.0776} & \textbf{88.70} & \textbf{0.0472} \\
5.6234 & 103.45 & 0.0981 & 95.27 & 0.0485 & 90.05 & 0.0181 \\
10.0 & 104.84 & 0.0736 & 96.24 & 0.0290 & 89.47 & 0.0306 \\
31.62 & 108.93 & 0.0000 & 97.68 & $-0.0003$ & 90.89 & $-0.0003$ \\
100.0 & 108.93 & 0.0000 & 97.68 & $-0.0003$ & 90.89 & $-0.0003$ \\
\hline
\end{tabular}
\end{center}
\noindent\textbf{Optimal parameters based on validation RMSE:}
\begin{itemize}
\item \textbf{Ridge:} I picked $\lambda_{\text{Ridge}}^{*} = 0.0001$ because it gave the lowest val RMSE of 97.54. Basically, for ridge, adding more regularization didn't help; the smallest lambda, which is almost like no regularization, worked best here on the training data alone. It tells me that ridge isn't adding much value beyond plain OLS in this setup.
\item \textbf{Lasso:} For lasso, $\lambda_{\text{Lasso}}^{*} = 3.1623$ stood out with val RMSE 93.81, way better than OLS or ridge because lasso zeros out noisy features, 16 of them in this case, focusing on the important ones and boosting generalization. It shows there are redundant or irrelevant variables in the mix, and lasso's sparsity helps clean that up.
\end{itemize}
\subsection*{(i) (Code)}
Similar to part 2d, write the Python functions, \texttt{eval\_ridge2(trainx, trainy, valx, valy, testx, testy, alpha)} and \texttt{eval\_lasso2(trainx, trainy, valx, valy, testx, testy, alpha)} that train ridge and lasso using the training and validation set.

\subsection*{(j) (Written)}
Use the optimal regularization parameter from 2h and report the RMSE and $R^{2}$ on the training set, validation set, and test set for the functions you wrote on 2i? How does this compare to the results from 2h? What do the numbers suggest?
\medskip

\noindent\textbf{Answer:}
Using the optimal $\lambda$ from 2h ($\lambda_{\text{Ridge}}^{*} = 0.0001$, $\lambda_{\text{Lasso}}^{*} = 3.1623$), the models are now retrained on the combined training + validation set:


\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
Ridge1 ($\lambda=0.0001$)    & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
Ridge2 ($\lambda=0.0001$)    & 99.38 & 0.1676 & 88.71 & 0.1751 &  86.41 &    0.0959 \\
\hline
Lasso1 ($\lambda=3.1623$)    & 100.87 & 0.1424 & 93.81 & 0.0776 & 88.70 & 0.0472 \\
Lasso2 ($\lambda=3.1623$)    & 101.67 & 0.1289 & 91.82 & 0.1163 & 86.64 & 0.0909 \\
\hline
\end{tabular}%
}
\end{center}


Retraining on the bigger set (train + val) really boosted things for both. For ridge, test RMSE dropped from 100.03 to 86.41, and R² went from negative to 0.10 clearly, more data helped it capture broader patterns without overfitting as much.

Lasso saw a similar lift, test RMSE from 88.70 to 86.64 and R² up to 0.091, but the gain was smaller since it was already doing okay with feature selection. Train RMSE went up a tad in both cases, which makes sense because the model's not tailoring itself as tightly to the original train data.

Overall, this screams bias-variance tradeoff: you're accepting a slight worse fit on train for much better performance elsewhere. Interesting how ridge and lasso end up neck-and-neck on test with more data; it suggests that once you have enough samples, the type of regularization matters less than just having the volume to learn robustly.


\subsection*{(k) (Written)}
Generate the coefficient path plots (regularization value vs. coefficient value) for both ridge and lasso. Also, note (line or point or star) where the optimal regularization parameters from 2h are on their respective plots. Make sure that your plots encompass all the expected behavior (coefficients should shrink towards 0).

\medskip
\noindent\textbf{Answer:}
The coefficient path plots were generated for $\lambda \in [10^{-4}, 10^{2}]$ using 50 log-spaced values. The optimal $\lambda$ from 2h is marked on each plot with a star. 

\textbf{Ridge coefficient path:} As $\lambda$ increases from $10^{-4}$ to $10^{2}$, all 25 coefficients shrink continuously towards zero, but none are ever exactly zero. At low $\lambda$, the dominant features are \texttt{T\_out} ($-67.3$), \texttt{Tdewpoint} ($+63.3$), \texttt{RH\_2} ($-60.3$), \texttt{RH\_3} ($+53.7$), and \texttt{RH\_1} ($+43.4$). These remain the primary coefficients across the path, though their magnitudes diminish, and by $\lambda = 100$, the largest remaining coefficients are \texttt{RH\_2} ($-47.8$), \texttt{RH\_3} ($+44.3$), with \texttt{RH\_1} ($+36.7$), while \texttt{T\_out} has shrunk to $-8.1$ and \texttt{Tdewpoint} to $+6.6$.

Thus, The optimal $\lambda^*_{\text{Ridge}} = 0.0001$ lies at the far left, where regularization has negligible impact.

\textbf{Lasso coefficient path:} Lasso exhibits sparsity by driving coefficients to exactly zero at distinct thresholds. At $\lambda = 10^{-4}$, all 25 features are active. By $\lambda \approx 0.087$, two features are eliminated. At the optimal $\lambda^*_{\text{Lasso}} = 3.1623$, 16 out of 25 features are zeroed out. The 9 surviving features are: \texttt{lights}, \texttt{RH\_1}, \texttt{RH\_2}, \texttt{T3}, \texttt{RH\_3}, \texttt{T5}, \texttt{T6}, \texttt{RH\_8}, and \texttt{RH\_out}. For $\lambda \geq 31.62$, all coefficients are zeroed, resulting in a model that predicts only the target mean.


\subsection*{(l) (Written)}
What are 3 observations you can draw from looking at the coefficient path plots, and the metrics? This should be different from your observations from 2e, 2h, and 2j.

\medskip
\noindent\textbf{Answer:}
\begin{enumerate}
    \item \textbf{Implicit Feature Selection:} Lasso performs automated feature selection by zeroing out coefficients, whereas Ridge only performs shrinkage. At the optimal Lasso $\lambda$, only 9 features remain, yet it outperforms the dense Ridge model on validation metrics,  showing that a sparse model can effectively reduce noise and improve generalization when the dataset contains irrelevant features.
    
    \item \textbf{Handling of Correlated Features:} Features such as \texttt{T\_out} and \texttt{Tdewpoint} are physically correlated. In the Ridge path, both retain moderate coefficients, sharing the predictive load. In contrast, Lasso zeros both out by $\lambda = 3.1623$ in favor of humidity features (\texttt{RH\_1}, \texttt{RH\_2}, \texttt{RH\_3}), showing Lasso's tendency to select a representative feature from a correlated group, whereas Ridge distributes weights among them.
    
    \item \textbf{Feature Importance Ranking:} The order in which Lasso eliminates features provides an interpretable importance ranking. Survivors like \texttt{lights} and \texttt{RH\_1} are individually strong predictors. Early dropouts like \texttt{Windspeed} and \texttt{Visibility} have weak predictive power. This ranking is a unique diagnostic tool for interpretation that is not available in standard Ridge or OLS regression.
\end{enumerate}


\section*{3. Predicting Appliance Energy Usage using SGD (50 pts)}
Consider the Appliances energy prediction Data set from the previous problem. A template file, \texttt{elastic.py}, defines a class \texttt{ElasticNet} that takes in the regularization parameters, \texttt{el} ($\lambda$), \texttt{alpha} ($\alpha$), \texttt{eta} ($\eta$) or the learning rate, the \texttt{batch} size (batch $\in [1, N]$), and \texttt{epoch} or the maximum number of epochs as parameters when creating the object (i.e., \texttt{elastic = new ElasticNet(el, alpha, eta, batch, epoch)}). You will implement \texttt{ElasticNet} using stochastic gradient descent to train your model. The functions in \texttt{'elastic.py'} will be tested against a different training, validation, and test set, so it should work for a variety of datasets and assume that the data has been appropriately pre-processed. For this problem, you \textbf{ARE NOT} allowed to use any existing toolbox/implementation (e.g., \texttt{scikit-learn}). Similar to problem 2, any additional work outside of (Code) should be done in a separate file and submitted with the Code for full credit.
\subsection*{(a) (Code)}
Implement the loss objective helper function in \texttt{elastic.py}. As a reminder, the optimization problem is:
$$
\min f_{o}(x) = \frac{1}{2} ||\mathbf{y} - \mathbf{X}\beta||_{2}^{2} + \lambda(\alpha||\beta||_{2}^{2} + (1-\alpha)||\beta||_{1}), \quad 0 < \alpha < 1 \quad (1)
$$
In other words, given the coefficients, data and the regularization parameters, your function will calculate the loss $f_{o}(x)$ as shown in Eq (1).
\subsection*{(b) (Code)}
Implement the gradient helper function in \texttt{elastic.py}. You may find it helpful to derive the update for a single training sample and to consider proximal gradient descent for the $||\beta||_{1}$ portion of the objective function. As a reminder, given step size $\eta$ and regularization parameter $\lambda$ such that $f(x) = g(x) + \lambda||x||_{1}$, the proximal update is:
$$
\text{prox}(x_i) =
\begin{cases}
x_i - \lambda\eta & \text{if } x_i > \lambda\eta \\
0 & \text{if } |x_i| \le \lambda\eta \\
x_i + \lambda\eta & \text{if } x_i < -\lambda\eta
\end{cases}
$$
\subsection*{(c) (Code)}
Implement the Python function \texttt{train(self, x, y)} for your class that trains an elastic net regression model using stochastic gradient descent. Your function should return a dictionary where the key denotes the epoch number and the value of the loss associated with that epoch. (Note: lambda is not used since it is a Python function and can cause confusion.)
\subsection*{(d) (Code)}
Implement the Python function \texttt{coef(self)} for your class that returns the learned coefficients as a numpy array.
\subsection*{(e) (Code)}
Implement the Python function \texttt{predict(self, x)} that predicts the label for each training sample in x. If x is a numpy $m \times d$ array, then y is a numpy 1-d array of size $m \times 1$.

\subsection*{(f) (Written)}
For the optimal regularization parameters from ridge $(\lambda_{ridge})$ and lasso $(\lambda_{lasso})$ from 2h, and $\alpha = \frac{1}{2}$, what are good learning rates for the dataset? Justify the selection by trying various learning rates and illustrating the objective value $(f_{o}(x))$ on a graph for a range of epochs (one epoch = one pass through the training data). For the chosen learning rate you identified, what are the RMSE and $R^{2}$ for the elastic net model trained on the entire training set on the training, validation, and test sets?

\medskip
\noindent\textbf{Answer:}
Using the optimal Lasso parameter $\lambda = 3.1623$ from 2h, with $\alpha = 0.5$ and a batch size of 32, the model was trained for 500 epochs. The following learning rates ($\eta$) were tested:

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{r r r r r r r r}
\hline
$\eta$ & Loss (ep 0) & Loss (ep 499) & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE \\
\hline
$10^{-3}$ & $9.92 \times 10^7$ & $9.75 \times 10^7$ & 140.59 & $-0.67$ & 126.86 & $-0.69$ & 101.48 \\
$10^{-4}$ & $9.99 \times 10^7$ & $9.67 \times 10^7$ & 140.01 & $-0.65$ & 130.24 & $-0.78$ & 113.62 \\
$10^{-5}$ & $1.05 \times 10^8$ & $9.67 \times 10^7$ & 139.99 & $-0.65$ & 131.17 & $-0.80$ & 116.99 \\
$10^{-6}$ & $1.06 \times 10^8$ & $9.74 \times 10^7$ & 140.53 & $-0.66$ & 133.38 & $-0.86$ & 123.32 \\
$\mathbf{10^{-7}}$ & $1.06 \times 10^8$ & $1.01 \times 10^8$ & 143.41 & $-0.73$ & \textbf{125.40} & $-0.65$ & 109.04 \\
\hline
\end{tabular}%
}
\end{center}

\noindent\textbf{Chosen learning rate: $\eta = 10^{-7}$.}

\noindent\textbf{Justification:} The learning rate $\eta = 10^{-7}$ was selected because it achieved the lowest Validation RMSE (125.40). The loss curve for this rate decreased steadily and monotonically without the oscillations observed at higher rates:

\begin{center}
\begin{tabular}{r r}
\hline
Epoch & Objective $f_o$ \\
\hline
0 & $1.064 \times 10^8$ \\
49 & $1.054 \times 10^8$ \\
99 & $1.047 \times 10^8$ \\
199 & $1.035 \times 10^8$ \\
499 & $1.013 \times 10^8$ \\
\hline
\end{tabular}
\end{center}

Larger learning rates (e.g., $10^{-3}$) reduced the loss faster initially but resulted in noisy updates due to mini-batch variance, leading to unstable convergence paths and worse final metrics. Since the loss function is defined as the \textit{sum} of squared errors (rather than the mean), the gradient magnitudes are very large ($\sim 10^8$). Given a Lipschitz constant of approximately $88,779$, the theoretical maximum $\eta$ for full Gradient Descent is $\approx 2.3 \times 10^{-5}$. With Stochastic Gradient Descent (SGD), the added noise necessitates an even smaller step size for stability.

\noindent\textbf{Final Metrics} (trained on the full training set with $\eta = 10^{-7}$):
\begin{center}
\begin{tabular}{l r r}
\hline
\textbf{Set} & \textbf{RMSE} & $\mathbf{R^2}$ \\
\hline
Train & 143.41 & $-0.7334$ \\
Validation & 125.40 & $-0.6484$ \\
Test & 109.04 & $-0.4399$ \\
\hline
\end{tabular}
\end{center}

The negative $R^2$ values indicate that the model has not fully converged after 500 epochs. The high condition number of the design matrix ($\kappa \approx 8,282$) means gradient-based methods require significantly more iterations to converge compared to Scikit-learn's exact solvers.

\subsection*{(g) (Written)}
Using the learning rate from the previous part, train elastic net (using only training data) for different values of $\alpha$ (it should encompass the entire range and include $\alpha = 0, 1$). Report the RMSE and $R^{2}$ for the models on training, validation, and test set.

\medskip
\noindent\textbf{Answer:}
Using $\eta=10^{-7}$, $\lambda=3.1623$, batch size 32, and 500 epochs, the Elastic Net was trained across $\alpha$ values from 0 to 1:

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{r r r r r r r}
\hline
$\alpha$ & Train RMSE & Train $R^2$ & Val RMSE & Val $R^2$ & Test RMSE & Test $R^2$ \\
\hline
0.00 (pure $L_1$) & 143.37 & $-0.7323$ & 125.33 & $-0.6466$ & 108.90 & $-0.4361$ \\
0.10 & 143.38 & $-0.7325$ & 125.35 & $-0.6470$ & 108.93 & $-0.4369$ \\
0.25 & 143.39 & $-0.7329$ & 125.37 & $-0.6475$ & 108.97 & $-0.4380$ \\
0.50 & 143.41 & $-0.7334$ & 125.40 & $-0.6484$ & 109.04 & $-0.4399$ \\
0.75 & 143.43 & $-0.7339$ & 125.43 & $-0.6493$ & 109.12 & $-0.4418$ \\
0.90 & 143.44 & $-0.7342$ & 125.45 & $-0.6498$ & 109.16 & $-0.4429$ \\
1.00 (pure $L_2$) & 143.45 & $-0.7344$ & 125.47 & $-0.6501$ & 109.19 & $-0.4437$ \\
\hline
\end{tabular}%
}
\end{center}

The results are highly similar across all $\alpha$ values, with Test RMSE ranging only from 108.90 to 109.19. The $\alpha=0$ model (pure Lasso-like) performs marginally better. This lack of variation is due to under-convergence; the coefficient norms ($\|\beta\|_2 \approx 15$) are small compared to the optimal Scikit-learn coefficients ($\approx 45-146$). When coefficients are small, the difference between $L_1$ and $L_2$ penalties is minimal, leading to similar optimization paths.

\subsection*{(h) (Written)}
Based on the results from (c) and 2(a) and 2(c), what conclusions can you draw in terms of RMSE and $R^{2}$? Which model is the best? Also, discuss the differences between the SGD-variants of Ridge and LASSO and the standard implementations (Problem 2).

\medskip
\noindent\textbf{Answer:}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l r r r r r r}
\hline
\textbf{Model} & \textbf{Train RMSE} & \textbf{Train $R^2$} & \textbf{Val RMSE} & \textbf{Val $R^2$} & \textbf{Test RMSE} & \textbf{Test $R^2$} \\
\hline
sklearn Ridge ($\lambda=0.0001$) & 98.23 & 0.1868 & 97.54 & 0.0027 & 100.03 & $-0.2117$ \\
sklearn Lasso ($\lambda=3.1623$) & 100.87 & 0.1424 & 93.81 & 0.0776 & 88.70 & 0.0472 \\
SGD $\alpha=1$ (Ridge-like) & 143.45 & $-0.7344$ & 125.47 & $-0.6501$ & 109.19 & $-0.4437$ \\
SGD $\alpha=0$ (Lasso-like) & 143.37 & $-0.7323$ & 125.33 & $-0.6466$ & 108.90 & $-0.4361$ \\
SGD $\alpha=0.5$ (Elastic Net) & 143.41 & $-0.7334$ & 125.40 & $-0.6484$ & 109.04 & $-0.4399$ \\
\hline
\end{tabular}%
}
\end{center}

\noindent\textbf{Best Model:} The \textbf{Scikit-learn Lasso} model at $\lambda = 3.1623$ is the best performing model, achieving the lowest Test RMSE (88.70) and a positive $R^2$ (0.0472). It significantly outperforms all SGD variants.

\noindent\textbf{Key Differences between SGD and Scikit-learn:}
\begin{itemize}
    \item \textbf{Loss Formulation:} Our SGD implementation uses the sum of squared errors. With $N=9868$, the data loss is $\sim 10^8$, making the regularization term ($\lambda$) effectively negligible. Scikit-learn averages the loss by $1/(2N)$, making the same $\lambda$ much more impactful.
    \item \textbf{Optimizer Efficiency:} Scikit-learn uses exact solvers (Ridge uses matrix inversion) or coordinate descent (Lasso), which converge to the global optimum very efficiently. SGD relies on noisy gradient estimates and fixed learning rates, limiting its precision.
    \item \textbf{Conditioning:} The design matrix condition number is high ($\kappa \approx 8,282$). Gradient descent requires $O(\kappa \log(1/\epsilon))$ iterations, making it slow to converge. Coordinate descent handles this sparsity more effectively.
    \item \textbf{Convergence State:} The SGD coefficients ($\|\beta\|_2 \approx 15$) are far smaller than the optimal Scikit-learn coefficients ($\approx 44-146$). The SGD model is effectively underfit, resulting in near-zero predictions and negative $R^2$ values.
\end{itemize}

\subsection*{(i) (Written)}
What are the final coefficients that yield the best elastic net model on the test data? Compare these with the final coefficients for the best-performing model on the validation dataset. Are there noticeable differences? If so, discuss the differences with respect to the impact on the performance.

\medskip
\noindent\textbf{Answer:}
The best performing model on both the Test and Validation sets was $\alpha=0$ (pure $L_1$). Since the hyperparameters and random seed were identical, the resulting model coefficients are identical:

\begin{center}
\small
\begin{tabular}{l r | l r}
\hline
\textbf{Feature} & \textbf{Coefficient} & \textbf{Feature} & \textbf{Coefficient} \\
\hline
lights & 10.4483 & T6 & 2.2838 \\
T1 & 0.6884 & RH\_6 & $-0.7428$ \\
RH\_1 & 3.2186 & T7 & $-0.3003$ \\
T2 & 4.1325 & RH\_7 & $-2.3318$ \\
RH\_2 & $-2.9227$ & T8 & 0.6047 \\
T3 & 3.5179 & RH\_8 & $-4.4550$ \\
RH\_3 & 2.1658 & T9 & $-1.2969$ \\
T4 & 1.1150 & RH\_9 & $-3.0644$ \\
RH\_4 & 0.0234 & T\_out & 1.7445 \\
T5 & $-1.1690$ & Press\_mm\_hg & $-0.7695$ \\
RH\_5 & $-0.7227$ & RH\_out & $-3.6775$ \\
Visibility & 0.0047 & Windspeed & 0.9500 \\
Tdewpoint & $-0.3360$ & & \\
\hline
\end{tabular}
\end{center}
The coefficient vector has a norm of $\approx 15.05$, and all 25 coefficients are non-zero.

\noindent\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Magnitude:} The SGD coefficients are much smaller (max $\approx 10.4$) than the Scikit-learn Lasso coefficients ($\approx 24.1$) or OLS ($\approx 63.3$). This confirms the model has not run for enough epochs to build up to the optimal magnitude.
    \item \textbf{Lack of Sparsity:} Despite using $\alpha=0$, the SGD model retains all 25 features. Scikit-learn's Lasso zeros out 16 features. This occurs because the SGD coefficients are still small; the proximal operator only thresholds a coefficient to zero when it crosses the origin during a step, which is rare when updates are small and slow.
    \item \textbf{Feature Rankings:} The relative importance is similar (e.g., \texttt{lights} and humidity features are dominant). However, SGD assigns non-trivial weights to features that Lasso eliminated (e.g., \texttt{T\_out}, \texttt{Press\_mm\_hg}), introducing noise.
    \item \textbf{Performance Impact:} The under-converged state results in poor predictive performance (Test RMSE 108.9) compared to the converged Lasso baseline (88.7). Implementing a learning rate schedule or increasing epochs significantly would be necessary to close this gap.
\end{itemize}
\end{document}


